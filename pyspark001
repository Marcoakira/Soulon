# -*- coding: utf-8 -*-
"""Untitled4.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1K0aIZgNB6ftmFGm8ui49_NpihYSbhsEH

** importando e montando RDD
"""

pip install pyspark

pip install findspark

import findspark

findspark.init()

import pyspark

from pyspark.sql import SparkSession

"""seção"""

spark = SparkSession.builder \
    .master("local") \
    .appName("Aula Introdutória PySpark") \
    .config("spark.executor.memory", "1gb") \
    .getOrCreate()

"""contexto"""

sc = spark.sparkContext

"""RDB"""

rdd = sc.textFile("/content/Salary_Dat_new.csv")
print(" importado com sucesso")



"""**Listagem de dados"""

rdd.take(20)

"""Ele quebra a super lista com varios elementos, em uma super lista com varias pequenas lista dentro"""

rdd = rdd.map(lambda line: line.split(","))

rdd.take(20)



"""**Trabalhando com colunas

importando a biblioteca que manipula colunas
"""

from pyspark.sql.types import Row

"""ele nomeia as colunas 0 como AnoExperiencia, e  a coluna como Salario"""

df = rdd.map(lambda line: Row(AnoExperiencia = line[0],Salario=line[1])).toDF()

"""ixibe o resultado alterado"""

df.show()

"""**alterando os tipo de dados, as colunas que estao em string vamos passar para float, que sera possivel manipular fazendo contas por exemplo"""

df.printSchema()

"""importar outra bibliotaca"""

from pyspark.sql.types import *

def converterColuna(dataframe, nomes, novoTipo):
  for nome in nomes:
    dataframe = dataframe.withColumn(nome, dataframe[nome].cast(novoTipo))
  return dataframe

colunas = ['AnoExperiencia','Salario']

df = converterColuna(df, colunas, FloatType())

df.show()

"""mostrando o novo schema"""

df.printSchema

"""
** Consultas simples
---

"""

df.select('Salario').show(10)

"""mostra uma coluna, quantas vezes aparece o valor, e em ordem descendente"""

df.groupBy('salario').count().sort('Salario', ascending=False).show()

df.select('Salario').count()

df.describe().show()

df.describe('Salario').show()

df.collect()

"""** consultas por condições"""

df.filter(df['salario'] > 40000).show(30)

"""aqui ele mostra só a coluna salario, com os valores passados"""

df.select('Salario').filter(df['Salario'] > 5000).show()

"""é um filtro multiplo, o sinal: | funciona como um and ou & ou e."""

df.filter((df['Salario'] > 5000) | (df['AnosExperiencia'] > 2)).show()

#df.filter(df['Salario'] > 5000).show()
#df.select('Salario').filter(df['Salario'] > 5000).show()
df.filter((df['Salario'] > 5000) | (df['AnosExperiencia'] > 2)).show()

"""** Definição de Schema / Novo dataset / nova forma de iniciar a aplicação

novas forma de importar
"""

from pyspark.sql.session import SparkSession

from pyspark.sql.types import (ArrayType, BooleanType, FloatType, IntegerType, StringType, StructField, StructType, TimestampType)

import pyspark.sql.functions as F

newspark = SparkSession.builder.appName('fisrtSession')\
    .config('spark.master', 'local[4]')\
    .config('spark.executor.memory', '1gb')\
    .config('spark.shuffle.partitions', 1)\
    .getOrCreate()

schema = StructType([
                StructField('case_id', IntegerType()),
                StructField('province', StringType()),
                StructField('city', StringType()),
                StructField('group', BooleanType()),
                StructField('infection_case', StringType()),
                StructField('confirmed', IntegerType()),
                StructField('latitude', StringType()),
                StructField('longitude', StringType())
])

path = "/content/covid_cases.csv"

df = spark.read.format('csv')\
    .schema(schema)\
    .load(path)

df.printSchema()
